{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abrham17/Diffusion_model-UNet-implementation/blob/main/Diffusion_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lbtY1yddOONI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import time\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NoWAgqmp2GK0"
      },
      "outputs": [],
      "source": [
        "def space_to_depth(x, size=2):\n",
        "    b, c, h, w = x.shape\n",
        "    out_h = h // size\n",
        "    out_w = w // size\n",
        "    out_c = c * (size * size)\n",
        "    x = x.reshape((-1, c, out_h, size, out_w, size))\n",
        "    x = x.permute((0, 1, 3, 5, 2, 4))\n",
        "    x = x.reshape((-1, out_c, out_h, out_w))\n",
        "    return x\n",
        "\n",
        "class SpaceToDepth(nn.Module):\n",
        "    def __init__(self, size):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "    def forward(self, x):\n",
        "        return space_to_depth(x, self.size)\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, func):\n",
        "        super().__init__()\n",
        "        self.func = func\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return x + self.func(x, *args, **kwargs)\n",
        "\n",
        "def upsample(in_channels, out_channels=None):\n",
        "    out_channels = in_channels if out_channels is None else out_channels\n",
        "    return nn.Sequential(\n",
        "        nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "        nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
        "    )\n",
        "\n",
        "def downsample(in_channels, out_channels=None):\n",
        "    out_channels = in_channels if out_channels is None else out_channels\n",
        "    return nn.Sequential(\n",
        "        SpaceToDepth(2),\n",
        "        nn.Conv2d(4 * in_channels, out_channels, 1)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1W3wFpOr2QDj"
      },
      "outputs": [],
      "source": [
        "class SinusodialPositionEmbedding(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "    def forward(self, time_steps):\n",
        "        positions = torch.unsqueeze(time_steps, 1)\n",
        "        half_dim = self.embedding_dim // 2\n",
        "        embeddings = torch.zeros((time_steps.shape[0], self.embedding_dim), device=time_steps.device)\n",
        "        denominators = 10_000 ** (2 * torch.arange(self.embedding_dim // 2, device=time_steps.device) / self.embedding_dim)\n",
        "        embeddings[:, 0::2] = torch.sin(positions/denominators)\n",
        "        embeddings[:, 1::2] = torch.cos(positions/denominators)\n",
        "        return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JtMBmpPC2k4y"
      },
      "outputs": [],
      "source": [
        "class WeightStandardizedConv2d(nn.Conv2d):\n",
        "    def forward(self, x):\n",
        "        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n",
        "        weight = self.weight\n",
        "        mean = weight.mean(dim=[1,2,3], keepdim=True)\n",
        "        variance = weight.var(dim=[1,2,3], keepdim=True, correction=0)\n",
        "        normalized_weight = (weight - mean) / torch.sqrt(variance + eps)\n",
        "        return F.conv2d(x, normalized_weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, groups=8):\n",
        "        super().__init__()\n",
        "        self.proj = WeightStandardizedConv2d(in_channels, out_channels, 3, padding=1)\n",
        "        self.norm = nn.GroupNorm(groups, out_channels)\n",
        "        self.act = nn.SiLU()\n",
        "    def forward(self, x, scale_shift=None):\n",
        "        x = self.proj(x)\n",
        "        x = self.norm(x)\n",
        "        if scale_shift is not None:\n",
        "            scale, shift = scale_shift\n",
        "            x = x * (scale + 1) + shift\n",
        "        x = self.act(x)\n",
        "        return x\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, time_emb_dim=None, groups=8):\n",
        "        super().__init__()\n",
        "        if time_emb_dim is not None:\n",
        "            self.mlp = nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, 2 * out_channels))\n",
        "        else:\n",
        "            self.mlp = None\n",
        "        self.block1 = Block(in_channels, out_channels, groups)\n",
        "        self.block2 = Block(out_channels, out_channels, groups)\n",
        "        self.res_conv = nn.Identity() if in_channels == out_channels else nn.Conv2d(in_channels, out_channels, 1)\n",
        "    def forward(self, x, time_emb=None):\n",
        "        scale_shift = None\n",
        "        if self.mlp is not None and time_emb is not None:\n",
        "            time_emb = self.mlp(time_emb)\n",
        "            time_emb = time_emb.view(*time_emb.shape, 1, 1)\n",
        "            scale_shift = time_emb.chunk(2, dim=1)\n",
        "        h = self.block1(x, scale_shift=scale_shift)\n",
        "        h = self.block2(h)\n",
        "        return h + self.res_conv(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, in_channels, num_heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.dim_head = dim_head\n",
        "        self.scale_factor = 1 / (dim_head) ** 0.5\n",
        "        self.hidden_dim = num_heads * dim_head\n",
        "        self.input_to_qkv = nn.Conv2d(in_channels, 3 * self.hidden_dim, 1, bias=False)\n",
        "        self.to_output = nn.Conv2d(self.hidden_dim, in_channels, 1)\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.input_to_qkv(x)\n",
        "        q, k, v = map(lambda t: t.view(b, self.num_heads, self.dim_head, h * w), qkv.chunk(3, dim=1))\n",
        "        q = q * self.scale_factor\n",
        "        sim = torch.einsum(\"b h c i, b h c j -> b h i j\", q, k)\n",
        "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
        "        attention = sim.softmax(dim=-1)\n",
        "        output = torch.einsum(\"b h i j, b h c j -> b h i c\", attention, v)\n",
        "        output = output.permute(0, 1, 3, 2).reshape((b, self.hidden_dim, h, w))\n",
        "        return self.to_output(output)\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, in_channels, num_heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.dim_head = dim_head\n",
        "        self.scale_factor = 1 / (dim_head) ** 0.5\n",
        "        self.hidden_dim = num_heads * dim_head\n",
        "        self.input_to_qkv = nn.Conv2d(in_channels, 3 * self.hidden_dim, 1, bias=False)\n",
        "        self.to_output = nn.Sequential(nn.Conv2d(self.hidden_dim, in_channels, 1), nn.GroupNorm(1, in_channels))\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.input_to_qkv(x)\n",
        "        q, k, v = map(lambda t: t.view(b, self.num_heads, self.dim_head, h * w), qkv.chunk(3, dim=1))\n",
        "        q = q.softmax(dim=-2)\n",
        "        k = k.softmax(dim=-1)\n",
        "        q = q * self.scale_factor\n",
        "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
        "        output = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
        "        output = output.view((b, self.hidden_dim, h, w))\n",
        "        return self.to_output(output)\n",
        "\n",
        "class PreGroupNorm(nn.Module):\n",
        "    def __init__(self, dim, func, groups=1):\n",
        "        super().__init__()\n",
        "        self.func = func\n",
        "        self.group_norm = nn.GroupNorm(groups, dim)\n",
        "    def forward(self, x):\n",
        "        x = self.group_norm(x)\n",
        "        x = self.func(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MCvm5-2Q9iH5"
      },
      "outputs": [],
      "source": [
        "class DiffusionUnet(nn.Module):\n",
        "    def __init__(self, dim, init_dim=None, output_dim=None, dim_mults=(1, 2, 4, 8), channels=1, resnet_block_groups=4, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.num_classes = num_classes\n",
        "        init_dim = init_dim if init_dim is not None else dim\n",
        "        self.init_conv = nn.Conv2d(self.channels, init_dim, Feminist)\n",
        "        dims = [init_dim] + [m * dim for m in dim_mults]\n",
        "        input_output_dims = list(zip(dims[:-1], dims[1:]))\n",
        "\n",
        "        time_dim = 4 * dim\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusodialPositionEmbedding(dim),\n",
        "            nn.Linear(dim, time_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(time_dim, time_dim)\n",
        "        )\n",
        "        self.class_embedding = nn.Embedding(num_classes, time_dim)\n",
        "\n",
        "        self.down_layers = nn.ModuleList([])\n",
        "        for ii, (dim_in, dim_out) in enumerate(input_output_dims, 1):\n",
        "            is_last = ii == len(input_output_dims)\n",
        "            self.down_layers.append(nn.ModuleList([\n",
        "                ResnetBlock(dim_in, dim_in, time_emb_dim=time_dim, groups=resnet_block_groups),\n",
        "                ResnetBlock(dim_in, dim_in, time_emb_dim=time_dim, groups=resnet_block_groups),\n",
        "                Residual(PreGroupNorm(dim_in, LinearAttention(dim_in))),\n",
        "                downsample(dim_in, dim_out) if not is_last else nn.Conv2d(dim_in, dim_out, 3, padding=1)\n",
        "            ]))\n",
        "\n",
        "        mid_dim = dims[-1]\n",
        "        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, time_emb_dim=time_dim, groups=resnet_block_groups)\n",
        "        self.mid_attention = Residual(PreGroupNorm(mid_dim, Attention(mid_dim)))\n",
        "        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, time_emb_dim=time_dim, groups=resnet_block_groups)\n",
        "\n",
        "        self.up_layers = nn.ModuleList([])\n",
        "        for ii, (dim_in, dim_out) in enumerate(reversed(input_output_dims), 1):\n",
        "            is_last = ii == len(input_output_dims)\n",
        "            self.up_layers.append(nn.ModuleList([\n",
        "                ResnetBlock(dim_out + dim_in, dim_out, time_emb_dim=time_dim, groups=resnet_block_groups),\n",
        "                ResnetBlock(dim_out + dim_in, dim_out, time_emb_dim=time_dim, groups=resnet_block_groups),\n",
        "                Residual(PreGroupNorm(dim_out, LinearAttention(dim_out))),\n",
        "                upsample(dim_out, dim_in) if not is_last else nn.Conv2d(dim_out, dim_in, 3, padding=1)\n",
        "            ]))\n",
        "\n",
        "        self.output_dim = output_dim if output_dim is not None else channels\n",
        "        self.final_res_block = ResnetBlock(2 * dim, dim, time_emb_dim=time_dim, groups=resnet_block_groups)\n",
        "        self.final_conv = nn.Conv2d(dim, self.output_dim, 1)\n",
        "\n",
        "    def forward(self, x, time, class_labels):\n",
        "        x = self.init_conv(x)\n",
        "        init_result = x.clone()\n",
        "        time_emb = self.time_mlp(time)\n",
        "        class_emb = self.class_embedding(class_labels)\n",
        "        t = time_emb + class_emb\n",
        "\n",
        "        h = []\n",
        "        for block1, block2, attention, downsample_block in self.down_layers:\n",
        "            x = block1(x, t)\n",
        "            h.append(x)\n",
        "            x = block2(x, t)\n",
        "            x = attention(x)\n",
        "            h.append(x)\n",
        "            x = downsample_block(x)\n",
        "\n",
        "        x = self.mid_block1(x, t)\n",
        "        x = self.mid_attention(x)\n",
        "        x = self.mid_block2(x, t)\n",
        "\n",
        "        for block1, block2, attention, upsample_block in self.up_layers:\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = block1(x, t)\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = block2(x, t)\n",
        "            x = attention(x)\n",
        "            x = upsample_block(x)\n",
        "\n",
        "        x = torch.cat((x, init_result), dim=1)\n",
        "        x = self.final_res_block(x, t)\n",
        "        x = self.final_conv(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ULsHyKZOZ3mN"
      },
      "outputs": [],
      "source": [
        "def linear_schedule(num_timesteps):\n",
        "    beta_start = 1e-4\n",
        "    beta_end = 0.02\n",
        "    betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
        "    betas = torch.cat((torch.tensor([0]), betas))\n",
        "    return betas\n",
        "\n",
        "num_timesteps = 200\n",
        "betas_t = linear_schedule(num_timesteps)\n",
        "alphas_t = 1. - betas_t\n",
        "alphas_bar_t = torch.cumprod(alphas_t, dim=0)\n",
        "alphas_bar_t_minus_1 = torch.cat((torch.tensor([0]), alphas_bar_t[:-1 ]))\n",
        "one_over_sqrt_alphas_t = 1. / torch.sqrt(alphas_t)\n",
        "sqrt_alphas_t = torch.sqrt(alphas_t)\n",
        "sqrt_alphas_bar_t = torch.sqrt(alphas_bar_t)\n",
        "sqrt_alphas_bar_t_minus_1 = torch.sqrt(alphas_bar_t_minus_1)\n",
        "sqrt_1_minus_alphas_bar_t = torch.sqrt(1. - alphas_bar_t)\n",
        "posterior_variance = (1. - alphas_bar_t_minus_1) / (1. - alphas_bar_t) * betas_t\n",
        "\n",
        "def sample_by_t(tensor_to_sample, timesteps, x_shape):\n",
        "    batch_size = timesteps.shape[0]\n",
        "    sampled_tensor = tensor_to_sample.gather(-1, timesteps.cpu())\n",
        "    sampled_tensor = torch.reshape(sampled_tensor, (batch_size,) + (1,) * (len(x_shape) - 1))\n",
        "    return sampled_tensor.to(timesteps.device)\n",
        "\n",
        "def sample_q(x0, t, noise=None):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x0)\n",
        "    sqrt_alphas_bar_t_sampled = sample_by_t(sqrt_alphas_bar_t, t, x0.shape)\n",
        "    sqrt_1_minus_alphas_bar_t_sampled = sample_by_t(sqrt_1_minus_alphas_bar_t, t, x0.shape)\n",
        "    return sqrt_alphas_bar_t_sampled * x0 + sqrt_1_minus_alphas_bar_t_sampled * noise"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 32\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CwW6MyFd08D",
        "outputId": "5567498f-fc56-496b-e721-a29040163b25"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 40.1MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.07MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 9.45MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 11.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(model, x0, t, class_labels, noise=None):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x0)\n",
        "    x_t = sample_q(x0, t, noise)\n",
        "    predicted_noise = model(x_t, t, class_labels)\n",
        "    return F.l1_loss(noise, predicted_noise)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = DiffusionUnet(dim=32, channels=1, dim_mults=(1, 2, 4, 8)).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "results_folder = Path(\"./results\")\n",
        "results_folder.mkdir(exist_ok=True)\n",
        "\n",
        "epochs = 20  # Reduced for demonstration\n",
        "loss_steps = 50\n",
        "sample_every = 500\n",
        "loss_for_mean = np.zeros(loss_steps)\n",
        "prev_time = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for batch_index, (images, labels) in enumerate(train_dataloader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        t = torch.randint(1, num_timesteps, (images.shape[0],), device=device).long()\n",
        "        loss = compute_loss(model, images, t, labels)\n",
        "        current_step = batch_index + epoch * len(train_dataloader)\n",
        "\n",
        "        if current_step % loss_steps == 0:\n",
        "            batches_left = epochs * len(train_dataloader) - current_step\n",
        "            time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time) / loss_steps)\n",
        "            print(f'Epoch {epoch}, Batch {batch_index}: Loss {loss_for_mean.mean():.4f}, Time left: {time_left}')\n",
        "            prev_time = time.time()\n",
        "            loss_for_mean[:] = 0\n",
        "\n",
        "        loss_for_mean[current_step % loss_steps] = loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if current_step % sample_every == 0:\n",
        "            class_labels = torch.randint(0, 10, (5,), device=device)\n",
        "            sample_images_list = sampling(model, (5, 1, image_size, image_size), class_labels)\n",
        "            save_images(sample_images_list[-1], f'sample_{current_step}.png')"
      ],
      "metadata": {
        "id": "25mPq1f7d2tP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def sample_p(model, x_t, t, class_labels, clipping=True):\n",
        "    betas_t_sampled = sample_by_t(betas_t, t, x_t.shape)\n",
        "    sqrt_1_minus_alphas_bar_t_sampled = sample_by_t(sqrt_1_minus_alphas_bar_t, t, x_t.shape)\n",
        "    one_over_sqrt_alphas_t_sampled = sample_by_t(one_over_sqrt_alphas_t, t, x_t.shape)\n",
        "\n",
        "    if clipping:\n",
        "        sqrt_alphas_bar_t_sampled = sample_by_t(sqrt_alphas_bar_t, t, x_t.shape)\n",
        "        sqrt_alphas_bar_t_minus_1_sampled = sample_by_t(sqrt_alphas_bar_t_minus_1, t, x_t.shape)\n",
        "        alphas_bar_t_sampled = sample_by_t(alphas_bar_t, t, x_t.shape)\n",
        "        sqrt_alphas_t_sampled = sample_by_t(sqrt_alphas_t, t, x_t.shape)\n",
        "        alphas_bar_t_minus_1_sampled = sample_by_t(alphas_bar_t_minus_1, t, x_t.shape)\n",
        "\n",
        "        x0_reconstruct = 1 / sqrt_alphas_bar_t_sampled * (x_t - sqrt_1_minus_alphas_bar_t_sampled * model(x_t, t, class_labels))\n",
        "        x0_reconstruct = torch.clip(x0_reconstruct, -1., 1.)\n",
        "        predicted_mean = (sqrt_alphas_bar_t_minus_1_sampled * betas_t_sampled) / (1 - alphas_bar_t_sampled) * x0_reconstruct + \\\n",
        "                         (sqrt_alphas_t_sampled * (1 - alphas_bar_t_minus_1_sampled)) / (1 - alphas_bar_t_sampled) * x_t\n",
        "    else:\n",
        "        predicted_mean = one_over_sqrt_alphas_t_sampled * (x_t - betas_t_sampled / sqrt_1_minus_alphas_bar_t_sampled * model(x_t, t, class_labels))\n",
        "\n",
        "    if t[0].item() == 1:\n",
        "        return predicted_mean\n",
        "    else:\n",
        "        posterior_variance_sampled = sample_by_t(posterior_variance, t, x_t.shape)\n",
        "        noise = torch.randn_like(x_t)\n",
        "        return predicted_mean + torch.sqrt(posterior_variance_sampled) * noise\n",
        "\n",
        "@torch.no_grad()\n",
        "def sampling(model, shape, class_labels, image_noise_steps_to_keep=10):\n",
        "    batch = shape[0]\n",
        "    images = torch.randn(shape, device=device)\n",
        "    images_list = []\n",
        "    for timestep in tqdm(range(num_timesteps, 0, -1), desc='Sampling'):\n",
        "        t = torch.full((batch,), timestep, device=device, dtype=torch.long)\n",
        "        images = sample_p(model, images, t, class_labels)\n",
        "        if timestep <= image_noise_steps_to_keep:\n",
        "            images_list.append(images.cpu())\n",
        "    return images_list\n",
        "\n",
        "def save_images(images, filename):\n",
        "    images = (images.clamp(-1, 1) + 1) / 2\n",
        "    grid = make_grid(images, nrow=5)\n",
        "    torchvision.utils.save_image(grid, str(results_folder / filename))\n",
        "\n",
        "def plot_intermediate_steps(images_list, class_label):\n",
        "    fig, axes = plt.subplots(1, len(images_list), figsize=(15, 3))\n",
        "    for i, images in enumerate(images_list):\n",
        "        images = (images.clamp(-1, 1) + 1) / 2\n",
        "        grid = make_grid(images, nrow=images.shape[0])\n",
        "        axes[i].imshow(grid.permute(1, 2, 0).numpy().squeeze(), cmap='gray')\n",
        "        axes[i].axis('off')\n",
        "        axes[i].set_title(f'Step {num_timesteps - i * (num_timesteps // len(images_list))}')\n",
        "    plt.suptitle(f'Class {class_label}')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "xPY0ynMhd7-s"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for class_label in range(10):\n",
        "    class_labels = torch.full((4,), class_label, device=device)\n",
        "    sample_images_list = sampling(model, (4, 1, image_size, image_size), class_labels)\n",
        "    plot_intermediate_steps(sample_images_list, class_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "ZgYeW222eDP8",
        "outputId": "f9ad28b6-50f8-4c27-fdda-921bfb3c846f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-11-2565360749.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mclass_label\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mclass_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msample_images_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mplot_intermediate_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_images_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "authorship_tag": "ABX9TyOD1b1Bx7Ntca00oISucmrW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}